{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Chorismate mutase](https://en.wikipedia.org/wiki/Chorismate_mutase) is an enzyme used by bacteria, archaea, fungi, and plants to speed up the synthesis of two amino-acid tyrosine and phenylalanine (one of the essential ones). Given the huge increase of the reaction rate provided by CM, bacteria lacking CM can grow only if tyrosine and phenylalanine are supplemented to their growth media.\n",
    "\n",
    "The dataset `chorismate_alignment.faa` is a Multiple Sequence Alignment (MSA) of the AroQ family of CMs, taken from [a recent paper](https://www.science.org/doi/full/10.1126/science.aba3304) (basically it is Supplementary Table 1), and it has been built by *aligning* the CMs from several organisms. Aligning here means that *gaps* are added to the amino-acids of the protein sequences so that [amino-acid conserved across different organisms are vertically aligned in the MSA](https://en.wikipedia.org/wiki/Sequence_alignment).\n",
    "\n",
    "In this tutorial, we will train several generative models with this dataset. To test the trained models, we will two other datasets, `functional_seqs.faa` and `nonfunctional_seqs.faa` which contain some artificially generated sequences which have been shown to allow (functional sequences) or not (nonfunctional sequences) bacterial growth in *in vivo* assays.\n",
    "\n",
    "#### Task 1: collect training sequences\n",
    "Fasta formatted files (usual extensions for these files are .fasta, or .fa, or other which contains \"fa\") have a standard form: they can be divided in pairs of consecutive lines, of which the first starts with the symbol '>' and then there is some free text which is the label of the first sequence, and the second line is the sequence. \n",
    "The training dataset that we will use is in fasta format, and it is technically a multiple sequence alignment, that is the sequences contained in it share the same length and have been aligned in such a way that similar parts of the sequences have the same column indexes.\n",
    "The first task for this part of the tutorial is to read the training dataset and obtain all the sequences contained in it. \n",
    "\n",
    "#### Task 2: compute conservations\n",
    "Let us now give a look at the sequences. We can start from looking at each column of the multiple sequence alignment, and compute the conservation. As a first step, a good choice is to go from nucleotides to numbers, so that the list of sequences will become a rectangular matrix of integer numbers. Then, we can compute for each column $i$ the conservation $k_i$, defined as:\n",
    "$$ k_i = \\log_2(q) - H(f_i) = \\log_2(q) + \\sum_n f_i(n) \\log_2(f_i(n)) ,$$\n",
    "where $q$ is the number of symbols allowed in the sequence and $f_i(n)$ is the frequency of symbol $n$ in position $i$.\n",
    "Once we have computed the conservation for each position, we can plot them to visually see which parts of the sequneces are more conserved in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(file):\n",
    "    \"Read fasta file, returning (descriptors, sequences).\"\n",
    "    seqs = []\n",
    "    labels = []\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        t_s = \"\"\n",
    "        for l in lines:\n",
    "            t_l = l.rstrip() # remove \\n\n",
    "            if t_l[0] == '>':\n",
    "                if len(t_s) > 0:\n",
    "                    seqs.append(t_s)   \n",
    "                labels.append(t_l[1:])\n",
    "                t_s = \"\"\n",
    "            else:\n",
    "                t_s = t_s + t_l\n",
    "        seqs.append(t_s)    \n",
    "    return labels, seqs\n",
    "\n",
    "def seq2num(seq):\n",
    "    \"\"\"Transform a sequence or list of sequences\n",
    "    (seq) into a list of numbers.\"\"\"\n",
    "    AAs = ['-', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    ntdict = {AAs[k]:k for k in range(len(AAs))}\n",
    "    if type(seq) == str:\n",
    "        return np.array([ntdict[x] for x in seq], dtype=np.int16)[np.newaxis,:]\n",
    "    elif type(seq) == list:\n",
    "        return np.array([[ntdict[x] for x in seq_] for seq_ in seq], dtype=np.int16)\n",
    "    \n",
    "def num2seq(num):\n",
    "    \"Inverse of seq2num.\"\n",
    "    AAs = ['-', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    return ''.join([AAs[x] for x in num])\n",
    "    \n",
    "def seq2occurrence(seqs_AA, q_true=21):\n",
    "    \"\"\"Expand the alignment in an array of binary\n",
    "    occurrence variables. Moreover, fix the gauge \n",
    "    by removing the last symbol.\"\"\"\n",
    "    seqs = seq2num(seqs_AA)\n",
    "    B = seqs.shape[0]\n",
    "    N = seqs.shape[1]\n",
    "    q_gauge = q_true-1\n",
    "    msa_exp = np.zeros((B, N * q_gauge), dtype=np.int16)\n",
    "    for i in range(N):\n",
    "        for a in range(q_gauge):\n",
    "            msa_exp[:,i*q_gauge+a] = (seqs[:,i] == a)\n",
    "    return msa_exp\n",
    "\n",
    "def conservation_1hot(freqs, q):\n",
    "    \"\"\"Return the conservation, measured in bits. Freqs must be \n",
    "    a numpy array of frequencies of 1-hot encoded variables after\n",
    "    gauge fixing by removing the last symbol.\"\"\"\n",
    "    reshaped_freqs = freqs.reshape(freqs.shape[0] // (q-1), q-1)\n",
    "    lastAA_freqs = 1 - np.sum(reshaped_freqs, axis=1)\n",
    "    conss = []\n",
    "    for i, f in enumerate(reshaped_freqs):\n",
    "        ff = f[f != 0]\n",
    "        lf = lastAA_freqs[i]\n",
    "        if lf != 0:\n",
    "            conss.append(np.log2(q) + np.sum(ff * np.log2(ff)) + lf * np.log2(lf))\n",
    "        else:\n",
    "            conss.append(np.log2(q) + np.sum(ff * np.log2(ff)))\n",
    "    return conss\n",
    "\n",
    "def seq_from_1hot_gauged(seq1hot, q_true = 21):\n",
    "    seq2num = []\n",
    "    for i in range(len(seq1hot)//(q_true-1)):\n",
    "        t_piece = seq1hot[i*(q_true-1):(i+1)*(q_true-1)]\n",
    "        where = (t_piece == 1)\n",
    "        if np.sum(where) == 0:\n",
    "            seq2num.append(q_true-1)\n",
    "        elif np.sum(where) == 1:\n",
    "            seq2num.append(np.arange(q_true-1)[where][0])\n",
    "        else:\n",
    "            print(\"problem!\")\n",
    "    return num2seq(seq2num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute frequencies, conservations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot conservations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will train the simplest possible model, an independent site model, to try to discriminate between functional and non-functional sequences. Notice that the training is completely unsupervised, as we will use the training set which is not annotated (we expect though that most of these sequences are functional).\n",
    "Our model will be of the form\n",
    "$$ p(s) = \\frac{1}{Z} e^{-E(s)}, \\quad E(s) = - \\sum_{i,a} h_i(a) \\, s_i(a), $$\n",
    "where $p(s)$ is the probability of the sequence $s$ according to the model, $h_i$ are the \"local fields\", that is the parameters to be inferred, where $s_{i}(a)$ is a categorical (1-hot) variable which is 1 if the symbol $a$ is present in position $i$ in the sequence $s$, 0 otherwise. $Z$ is a normalization.\n",
    "This model maximizes the entropy under a set of constraints of the frequences, which fix the fields so that the 1-point correlation functions of the models correspond to the frequencies observed in the data.\n",
    "\n",
    "The partition function is\n",
    "$$ Z = \\prod_i \\left(1 + \\sum_a e^{h_i(a)} \\right) $$\n",
    "and the maximization of the log-likelihood of the observed data gives\n",
    "$$ h_i(a) = \\log(f_i(a)) - \\log(f_i(g)), $$\n",
    "where $g$ is the amino-acid excluded by the explicit choice of the gauge we made with the 1-hot encoding (in our case, Y).\n",
    "\n",
    "Notice, however, that there are some positions where some symbols are never observed. In principle, this would give an unrealistic minus-infinity local field, that we want to avoid. For this reason, we add a regularization in the form of a pseudo-count as follows. Consider that, to our dataset of size $M$, we add a purely random dataset of size $\\alpha/(1-\\alpha) M$. The new frequencies becomes:\n",
    "$$ f_i(n) \\to \\tilde{f}_i(n) = (1-\\alpha) f_i(n) + \\frac{\\alpha}{q}  $$\n",
    "\n",
    "#### Task 4: fit an independent-site model\n",
    "* Find the fields of a independent-site model, and use $\\alpha=0.5$ to regularize through the introduction of pseudo-counts.\n",
    "* Read the test datasets, which are divided in positive (functional) and negative (non-functional) examples.\n",
    "* Use the independent-site model to compute the energy of each test sequence, and check whether the independent-site model model can discriminate the functional from the non-functinoal. Use the AUROC as a quantification of the performance (see the `metrics` package from `sklearn`).\n",
    "\n",
    "#### Task 5: sample from independent-site model to check 1-pt and 2-pt correlations\n",
    "The model we just fit can be used to generate new sequences. Indeed, it is enough to sample from the probability distribution that the model defines. Use the provided function to generate new sequences from this model, and check the agreement with the 1-pt and 2-pt correlations computed from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indip_model_fields(freqs, alpha=0, q=21):\n",
    "    \"\"\"Fit the independent site model in the 1-hot encoding gauge.\n",
    "    Add pseudocounts, equivalent to a dataset fraction \n",
    "    alpha/(1-alpha) of random sequences added to the dataset.\"\"\"\n",
    "    L = len(freqs)//(q-1)\n",
    "    pc_freqs = freqs * (1 - alpha) + alpha / q\n",
    "    freq_gauge = np.zeros_like(pc_freqs)\n",
    "    for i in range(L):\n",
    "        freq_gauge[i*(q-1):(i+1)*(q-1)] = 1-np.sum(pc_freqs[i*(q-1):(i+1)*(q-1)])\n",
    "    return np.log(pc_freqs/freq_gauge)\n",
    "\n",
    "def energy_indip_model(fields, seqs):\n",
    "    \"\"\"Compute the energy of the sequences seqs according to the indepentend site model.\"\"\"\n",
    "    return -np.dot(seqs,fields)\n",
    "\n",
    "def sample_data_indip(fields, beta=1, q=21):\n",
    "    \"\"\"Generate new data by sampling the independent site model.\"\"\"\n",
    "    L = len(fields)//(q-1)\n",
    "    new_data = np.zeros_like(fields)\n",
    "    for i in range(L):\n",
    "        freqs = np.ones(q)\n",
    "        freqs[:-1] = np.exp( beta * fields[i*(q-1):(i+1)*(q-1)] )\n",
    "        freqs = freqs / freqs.sum()\n",
    "        a = np.random.choice(q, p=freqs)\n",
    "        if a != q-1:\n",
    "            new_data[i*(q-1) + a] = 1\n",
    "    return new_data\n",
    "\n",
    "def compute_correlation_matrix(data_points):\n",
    "    \"\"\"Compute correlation matrix, provided that the\n",
    "    data points are given as binary occurrence variables.\"\"\"\n",
    "    M = data_points.shape[0]\n",
    "    x = data_points.astype(np.float32)\n",
    "    xt = x.transpose()\n",
    "    c = np.dot(xt, x) / M\n",
    "    return c\n",
    "\n",
    "\n",
    "def add_pseudocounts(corr_mat, freqs, alpha, q_true=21):\n",
    "    \"\"\"Add pseudocounts according to the \n",
    "    parameter alpha to the frequencies and the correlation matrix,\n",
    "    and return them.\"\"\"\n",
    "    q_gauge = q_true - 1\n",
    "    N = len(freqs) // q_gauge\n",
    "    f_pseudo = (1-alpha)*freqs + alpha/q_true\n",
    "    c_pseudo = (1-alpha)*corr_mat + alpha/(q_true**2)\n",
    "    #Fix the conservation constraints in the diagonal blocks\n",
    "    for i in range(N):\n",
    "        c_pseudo[i*q_gauge:(i+1)*q_gauge,i*q_gauge:(i+1)*q_gauge] = 0\n",
    "        for a in range(q_gauge):\n",
    "            c_pseudo[i*q_gauge+a,i*q_gauge+a] = f_pseudo[i*q_gauge+a]\n",
    "    return f_pseudo, c_pseudo\n",
    "\n",
    "\n",
    "def connect_correlation(corr_mat, freqs):\n",
    "    \"\"\"Given the correlation matrix and the frequencies,\n",
    "    return the connected correlation matrix.\"\"\"\n",
    "    c_connected = corr_mat - np.outer(freqs, freqs)\n",
    "    return c_connected\n",
    "\n",
    "def flatten_correlation_matrix(corr,q=21):\n",
    "    L = int(corr.shape[0] / (q-1))\n",
    "    flatten = []\n",
    "    for i in range(L):\n",
    "        for a in range(q-1):\n",
    "            flatten = np.r_[flatten,corr[i*(q-1)+a,(i+1)*(q-1):]]\n",
    "    return flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"train\" model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute energies test sets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram and compute AUROC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from independent model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute 1-pt correlations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute 2-pt correlations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare 1-pt and 2-pt correlations from train and sampled data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to models able to capture 2-point correlations. We consider the model (Boltzmann Machine):\n",
    "$$ p(x) = \\frac{1}{Z} e^{-E(s)}, \\quad E(s) = \\sum_{i, a, j, b} s_{i}(a) \\, J_{i,j}(a,b) \\, s_{j}(b) + \\sum_{i, a} \\, h_i(a) s_i(a), $$\n",
    "where $s_{i}(a)$ is a categorical (1-hot) variable which is 1 if the symbol $a$ is present in position $i$ in the sequence $s$, 0 otherwise.\n",
    "In this case the exact inference of the model (that is, the problem of maximizing the log-likelihood of the data) cannot be solved exactly. We will consider here the Gaussian approximation, which allows the problem to be solved easily provided that the categorical variables have zero average. To do that, we consider the new model:\n",
    "$$ p(x) = \\frac{1}{Z} e^{-E(\\tilde{s})}, \\quad E(\\tilde{s}) = \\sum_{i, a, j, b} \\tilde{s}_{i}(a) \\, J_{i,j}(a,b) \\, \\tilde{s}_{j}(b) + \\sum_{i, a} \\, h_i(a) \\tilde{s}_i(a), $$\n",
    "where $\\tilde{s}_i(a) = s_i - f_i(a)$. Notice that, once we know how to infer the parameters for this new model, the old one can be immediately recovered by inserting the expression for the variables $\\tilde{s}_i(a)$.\n",
    "The Gaussian approximation consists in neglecting the binary nature of the categorical variables, and in computing the parameters $J_{i,j}(a,b)$ and $h_i(a)$ as if they were continuous variables. The relevant advantage in this case is that the normalization $Z$ can be computed exactly and derived with respect to the parameters during the maximization of the log-likelihood, and the solution is:\n",
    "$$ h_i(a) = 0, \\quad J_{i,j}(a,b) = - \\tilde{C}^{-1}_{ia, jb}, $$\n",
    "where $C^{-1}_{ia, jb}$ is the inverse of the correlation matrix of the (shifted) categorical variables, that is\n",
    "$$ \\tilde{C}_{ia, jb} = \\frac{1}{M} \\sum_s \\tilde{s}_i(a) \\tilde{s}_j(b), $$\n",
    "where $M$ is the number of sequences in the training dataset and the sum spans the sequences of the training dataset. Notice that this correlation matrix can be written as\n",
    "$$ \\tilde{C}_{ia, jb} = \\frac{1}{M} \\sum_s s_i(a) s_j(b) - f_i(a) f_j(b) = C_{ia, jb} - f_i(a) f_j(b), $$\n",
    "where $C_{ia, jb}$ is the correlation matrix of the original variables.\n",
    "\n",
    "Unfortunately, the correlation matrix as it is cannot be inverted. There are two reasons for that, that need to be addressed with two, separated considerations. Firstly, as in the indipendent-site case, a choice of gauge must be made to remove $L$ null eigenvalues from the correlation matrix. The easiest one in this case is to just neglect one of the symbols in each site. With a slight simplification, one can understand this because for each position $i$, even if there are $q$ symbols, only $q-1$ categorical variables are needed: if each of them is zero, it means that in position $i$ there is the only symbol for which we are not keeping the categorical variable.\n",
    "Even after the gauge fixing, the correlation matrix can still be not invertible. This happens when the number of data is low (lower than the lenght of each the sequence in categorical variables after gauge fixing) or if the data are correlated. In this case, we need again to rely on a regularization method to overcome the problem. We can again use pseudo-counts, transforming the frequences as we did for the indipendent-site model and the 2-point correlations as\n",
    "$$ f_{ij}(a,b) \\to \\tilde{f}_{ij}(a,b) = (1-\\alpha) f_{ij}(a,b) + \\frac{\\alpha}{q^2}. $$\n",
    "Unfortunately, this time a little extra trick is needed to fix things up after this transformation. Indeed, this transformation has the unwanted properties that now $\\tilde{f}_{ii}(a,b) \\neq 0$ for $a \\neq b$, and that $\\tilde{f}_{ii}(a,a) \\neq f_i(a)$. To get meaningful, reguralized frequencies and 2-point correlations, both of these properties must be re-imposed *a posteriori* after applying the transformation.\n",
    "\n",
    "#### Task 6: fit a Gaussian Boltzmann Machine\n",
    "* Go from our dataset written in numerical variables to a dataset written in categorial, binary variables. At this point, fix the gauge by neglecting one symbol for each position.\n",
    "* Compute the frequencies and the correlation matrix of these variables.\n",
    "* Apply the transformations discussed to both these object, and modify the correlation matrix by removing the matrix\n",
    "$ F_{ia, jb} = f_i(a) f_j(b) $ as discussed above.\n",
    "* Invert the resulting matrix (you can use the function `numpy.linalg.inv` from numpy) to get the $J$ matrix of the model.\n",
    "* Compute the energy of each sequence in the training set, and check whether the new model can discriminate between functional and non-functional sequences. Use the AUROC as a quantification of the performance (see the metrics package from sklearn). \n",
    "\n",
    "#### Task 7: sample from Gaussian Boltzmann Machine\n",
    "As for the independent-site model, the Gaussian Boltzmann Machine model define a probability distribution over the sequences, and hence can be used to generate new sequences. However, this is much more difficult due to the fact that $Z$ cannot be computed easily (when the variables are back to their binary nature), and that it is not known how to sample directly from the quadratic model (again, when the variables are binary). To overcome this problem, try to write Metropolis routine and sample from the model. \n",
    "\n",
    "#### Task 8: check (again) 1-pt and 2-pt correlations\n",
    "Similarly to above, compare the 1-pt and 2-pt correlations for the training data and those sampled with the Gaussian BM. Contrary to the independent-site model, the Gaussian BM is able to retrive both correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm_energy(jcoupling_gauss, ave_freqs, seq):\n",
    "    \"\"\"Compute energy of seq with the BM model.\"\"\"\n",
    "    # collect local fileds\n",
    "    h = np.dot(jcoupling_gauss, ave_freqs)\n",
    "    ener1 = - np.dot(h, seq)\n",
    "    ener2 = np.dot(seq, np.dot(jcoupling_gauss, seq)) / 2\n",
    "    return -(ener1 + ener2)\n",
    "\n",
    "\n",
    "\n",
    "def metropolis(jcoupl, p, Nsteps=10000, beta=1, q_true = 21, start_conf=np.nan):\n",
    "    \"\"\"Handle a single chain, starting in start_conf configuration, doing Nsteps steps \n",
    "    (each step is a single, random site flip).\"\"\"\n",
    "    L = len(p)\n",
    "    # prepare initial config\n",
    "    if np.isnan(np.sum(start_conf)):\n",
    "        s = np.random.randint(1, size=L)\n",
    "        start_conf = s.copy()\n",
    "    else:\n",
    "        if type(start_conf[0]) != np.int32:\n",
    "            print('Please use starting configuration in numeric form or use None for random start.')\n",
    "            return 1\n",
    "        else:\n",
    "            s = start_conf\n",
    "    # metropolis\n",
    "    for k in range(0, Nsteps):\n",
    "        # propose move\n",
    "        pos_t = np.random.randint(len(p)//(q_true-1))\n",
    "        t_n = np.random.randint(q_true)\n",
    "        t_1hot = np.repeat(0, q_true-1)\n",
    "        if t_n == q_true-1:\n",
    "            pass\n",
    "        else:\n",
    "            t_1hot[t_n] = 1\n",
    "        s_t = s.copy()\n",
    "        s_t[pos_t*(q_true-1):(pos_t+1)*(q_true-1)] = t_1hot\n",
    "        \n",
    "        \n",
    "        ## faster way to compute energy difference between configurations\n",
    "        h = np.dot(jcoupl[pos_t*(q_true-1):(pos_t+1)*(q_true-1)] , p)\n",
    "        delta_h = - np.dot( (s-s_t)[pos_t*(q_true-1):(pos_t+1)*(q_true-1)], h )\n",
    "        jcoupl_diag = jcoupl[pos_t*(q_true-1):(pos_t+1)*(q_true-1),pos_t*(q_true-1):(pos_t+1)*(q_true-1)]\n",
    "        jcoupl_marginal = np.dot( jcoupl[pos_t*(q_true-1):(pos_t+1)*(q_true-1)] , s)\n",
    "        delta_seq = (s-s_t)[pos_t*(q_true-1):(pos_t+1)*(q_true-1)]\n",
    "        delta_coupl1 = np.dot( delta_seq , jcoupl_marginal  )\n",
    "        delta_coupl2 = - 0.5 * np.dot( delta_seq, np.dot(jcoupl_diag, delta_seq)  )\n",
    "        energy_diff = delta_h + delta_coupl1 + delta_coupl2\n",
    "        \n",
    "        ## slower (but simpler to read) way to compute energy difference between configurations\n",
    "        #energy_diff = bm_energy(jcoupl, p, s_t) - bm_energy(jcoupl, p, s)\n",
    "        \n",
    "        # accept or not\n",
    "        prob = min(1, np.exp(- beta * (energy_diff)))\n",
    "        x = np.random.rand()\n",
    "        if x < prob:\n",
    "            s = s_t.copy()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute (non connected) correlation matrix\n",
    "\n",
    "\n",
    "# add pseudocount and take connected matrix\n",
    "\n",
    "\n",
    "# compute coupling matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute energies test sets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram and compute AUROC\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling from gaussian BM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute 1-pt and 2-pt correlation for data sampled with gaussian boltzmann machine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare 1-pt and 2-pt correlations from train and sampled data\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
