{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70de1a2",
   "metadata": {},
   "source": [
    "# part I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae371958",
   "metadata": {},
   "source": [
    "\n",
    "[Chorismate mutase](https://en.wikipedia.org/wiki/Chorismate_mutase) (CM) is an enzyme used by bacteria, archaea, fungi, and plants to speed up the synthesis of two amino-acids, tyrosine and phenylalanine. Given the huge increase of the reaction rate provided by CM, bacteria lacking CM can grow only if tyrosine and phenylalanine are supplemented to their growth media.\n",
    "\n",
    "The dataset `chorismate_alignment.faa` is a Multiple Sequence Alignment (MSA) of the AroQ family of CMs, taken from [a recent paper](https://www.science.org/doi/full/10.1126/science.aba3304) (basically its Supplementary Table 1), and it has been built by *aligning* the CMs from several organisms. Aligning here means that *gaps* are added to the amino-acids of the protein sequences so that [amino-acid conserved across different organisms are vertically aligned in the MSA](https://en.wikipedia.org/wiki/Sequence_alignment).\n",
    "\n",
    "In this tutorial, we will train several generative models with this dataset. To test the trained models, we will use two other datasets, `functional_seqs.faa` and `nonfunctional_seqs.faa` which contain some artificially generated sequences which have been shown to allow (functional sequences) or not (nonfunctional sequences) bacterial growth in *in vivo* assays.\n",
    "\n",
    "#### Task I.1: collect training sequences\n",
    "Fasta formatted files (usual extensions for these files are .fasta, or .fa, or other which contains \"fa\") have a standard form: they can be divided in pairs of consecutive lines, of which the first starts with the symbol '>' and then there is some free text which is the label of the first sequence, and the second line is the sequence. \n",
    "The training dataset that we will use is in fasta format, and it is technically a multiple sequence alignment, that is the sequences contained in it share the same length and have been aligned in such a way that similar parts of the sequences have the same column indexes.\n",
    "The first task for this part of the tutorial is to read the training dataset and have a look at some of the sequences contained in it. \n",
    "\n",
    "#### Task I.2: compute conservations\n",
    "Let us now give a look at some statistical properties of sequences. We can start from looking at each column of the multiple sequence alignment, and compute the conservation. \n",
    "As a first step, a good choice is to go from nucleotides to numbers, so that the list of sequences will become a rectangular matrix of integer numbers. Then, we can compute for each column $i$ the conservation $k_i$, defined as:\n",
    "$$ k_i = \\log_2(q) - H(f_i) = \\log_2(q) + \\sum_n f_i(n) \\log_2(f_i(n)) ,$$\n",
    "where $q$ is the number of symbols allowed in the sequence and $f_i(n)$ is the frequency of symbol $n$ in position $i$.\n",
    "In what follows we will need also to represent each CM sequence as a sequence of 0s and 1s (one-hot encoding). \n",
    "Moreover, we can reduce the number of binary (one-hot) variables per site by deciding, for instance, not to encode the last amino-acid. In this way, a series of zeroes means that the site is occupied by the last amino-acid in our ordering, so we are not losing generality (actually, we are fixing a \"gauge\" degree of freedom).\n",
    "For this task, you can decide whether to compute the conservation after one-hot encoding or before.\n",
    "Once we have computed the conservation for each position, we can plot them to visually see which parts of the sequneces are more conserved in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1324ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52793ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(file):\n",
    "    \"Read fasta file, returning (descriptors, sequences).\"\n",
    "    seqs = []\n",
    "    labels = []\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        t_s = \"\"\n",
    "        for l in lines:\n",
    "            t_l = l.rstrip() # remove \\n\n",
    "            if t_l[0] == '>':\n",
    "                if len(t_s) > 0:\n",
    "                    seqs.append(t_s)   \n",
    "                labels.append(t_l[1:])\n",
    "                t_s = \"\"\n",
    "            else:\n",
    "                t_s = t_s + t_l\n",
    "        seqs.append(t_s)    \n",
    "    return labels, seqs\n",
    "\n",
    "def seq2num(seq):\n",
    "    \"\"\"Transform a sequence or list of sequences\n",
    "    (seq) into a list of numbers.\"\"\"\n",
    "    AAs = ['-', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    ntdict = {AAs[k]:k for k in range(len(AAs))}\n",
    "    if type(seq) == str:\n",
    "        return np.array([ntdict[x] for x in seq], dtype=np.int16)[np.newaxis,:]\n",
    "    elif type(seq) == list:\n",
    "        return np.array([[ntdict[x] for x in seq_] for seq_ in seq], dtype=np.int16)\n",
    "    \n",
    "def num2seq(num):\n",
    "    \"Inverse of seq2num.\"\n",
    "    AAs = ['-', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    return ''.join([AAs[x] for x in num])\n",
    "    \n",
    "def seq2occurrence(seqs_AA, q_true=21):\n",
    "    \"\"\"Expand the alignment in an array of binary\n",
    "    occurrence variables. Moreover, fix the gauge \n",
    "    by removing the last symbol.\"\"\"\n",
    "    seqs = seq2num(seqs_AA)\n",
    "    B = seqs.shape[0]\n",
    "    N = seqs.shape[1]\n",
    "    q_gauge = q_true-1\n",
    "    msa_exp = np.zeros((B, N * q_gauge), dtype=np.int16)\n",
    "    for i in range(N):\n",
    "        for a in range(q_gauge):\n",
    "            msa_exp[:,i*q_gauge+a] = (seqs[:,i] == a)\n",
    "    return msa_exp\n",
    "\n",
    "def conservation_1hot(freqs, q):\n",
    "    \"\"\"Return the conservation, measured in bits. Freqs must be \n",
    "    a numpy array of frequencies of 1-hot encoded variables after\n",
    "    gauge fixing by removing the last symbol.\"\"\"\n",
    "    reshaped_freqs = freqs.reshape(freqs.shape[0] // (q-1), q-1)\n",
    "    lastAA_freqs = 1 - np.sum(reshaped_freqs, axis=1)\n",
    "    conss = []\n",
    "    for i, f in enumerate(reshaped_freqs):\n",
    "        ff = f[f != 0]\n",
    "        lf = lastAA_freqs[i]\n",
    "        if lf != 0:\n",
    "            conss.append(np.log2(q) + np.sum(ff * np.log2(ff)) + lf * np.log2(lf))\n",
    "        else:\n",
    "            conss.append(np.log2(q) + np.sum(ff * np.log2(ff)))\n",
    "    return conss\n",
    "\n",
    "def seq_from_1hot_gauged(seq1hot, q_true = 21):\n",
    "    seq2num = []\n",
    "    for i in range(len(seq1hot)//(q_true-1)):\n",
    "        t_piece = seq1hot[i*(q_true-1):(i+1)*(q_true-1)]\n",
    "        where = (t_piece == 1)\n",
    "        if np.sum(where) == 0:\n",
    "            seq2num.append(q_true-1)\n",
    "        elif np.sum(where) == 1:\n",
    "            seq2num.append(np.arange(q_true-1)[where][0])\n",
    "        else:\n",
    "            print(\"problem!\")\n",
    "    return num2seq(seq2num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdbcfce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99973d78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute frequencies, conservations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed7b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot conservations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f8f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f311c666",
   "metadata": {},
   "source": [
    "# part II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e70ad5",
   "metadata": {},
   "source": [
    "In this part we will train the simplest possible model, an independent site model, to try to discriminate between functional and non-functional sequences. Notice that the training is completely unsupervised, as we will use the training set which is not annotated (we expect though that most of these sequences are functional).\n",
    "Our model will be of the form\n",
    "$$ p(s) = \\frac{1}{Z} e^{-E(s)}, \\quad E(s) = - \\sum_{i,a} h_i(a) \\, s_i(a), $$\n",
    "where $p(s)$ is the probability of the sequence $s$ according to the model, $h_i$ are the \"local fields\", that is the parameters to be inferred, where $s_{i}(a)$ is a binary (one-hot) variable which is 1 if the symbol $a$ is present in position $i$ in the sequence $s$, 0 otherwise. $Z$ is a normalization.\n",
    "This model maximizes the entropy under a set of constraints on the single-site frequences, which fix the fields so that the 1-point correlation functions of the model correspond to the frequencies observed in the data.\n",
    "\n",
    "The partition function is\n",
    "$$ Z = \\prod_i \\left(1 + \\sum_a e^{h_i(a)} \\right) $$\n",
    "and the maximization of the log-likelihood of the observed data gives\n",
    "$$ h_i(a) = \\log(f_i(a)) - \\log(f_i(g)), $$\n",
    "where $g$ is the amino-acid excluded by the explicit choice of the gauge we made with the one-hot encoding (in our case, Y).\n",
    "\n",
    "Notice, however, that there are some positions where some symbols are never observed. In principle, this would give an unrealistic minus-infinity local field, that we want to avoid. For this reason, we add a regularization in the form of a pseudo-count as follows. Consider that, to our dataset of size $M$, we add a purely random dataset of size $\\alpha/(1-\\alpha) M$. The new frequencies becomes:\n",
    "$$ f_i(n) \\to \\tilde{f}_i(n) = (1-\\alpha) f_i(n) + \\frac{\\alpha}{q}  $$\n",
    "\n",
    "#### Task II.1: fit an independent-site model\n",
    "* Find the fields of a independent-site model, and use $\\alpha=0.01$ to regularize through the introduction of pseudo-counts.\n",
    "* Read the test datasets, which are divided in positive (functional) and negative (non-functional) examples.\n",
    "* Use the independent-site model to compute the energy of each test sequence, and check whether the independent-site model model can discriminate the functional from the non-functinoal. To do this, plot the histograms and use the AUROC as a quantification of the performance (see the `metrics` package from `sklearn`).\n",
    "\n",
    "#### Task II.2: sample from independent-site model to check 1-pt and 2-pt correlations\n",
    "The model we just fit can be used to generate new sequences. Indeed, it is enough to sample from the probability distribution that the model defines. Use the provided function to generate new sequences from this model, and check the agreement with the 1-pt and 2-pt (connected) correlations computed from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6bf83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb835c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indip_model_fields(freqs, alpha=0, q=21):\n",
    "    \"\"\"Fit the independent site model in the 1-hot encoding gauge.\n",
    "    Add pseudocounts, equivalent to a dataset fraction \n",
    "    alpha/(1-alpha) of random sequences added to the dataset.\"\"\"\n",
    "    L = len(freqs)//(q-1)\n",
    "    pc_freqs = freqs * (1 - alpha) + alpha / q\n",
    "    freq_gauge = np.zeros_like(pc_freqs)\n",
    "    for i in range(L):\n",
    "        freq_gauge[i*(q-1):(i+1)*(q-1)] = 1-np.sum(pc_freqs[i*(q-1):(i+1)*(q-1)])\n",
    "    return np.log(pc_freqs/freq_gauge)\n",
    "\n",
    "def energy_indip_model(fields, seqs):\n",
    "    \"\"\"Compute the energy of the sequences seqs according to the indepentend site model.\"\"\"\n",
    "    return -np.dot(seqs, fields)\n",
    "\n",
    "def sample_data_indip(fields, beta=1, q=21):\n",
    "    \"\"\"Generate new data by sampling the independent site model.\"\"\"\n",
    "    L = len(fields)//(q-1)\n",
    "    new_data = np.zeros_like(fields)\n",
    "    for i in range(L):\n",
    "        freqs = np.ones(q)\n",
    "        freqs[:-1] = np.exp( beta * fields[i*(q-1):(i+1)*(q-1)] )\n",
    "        freqs = freqs / freqs.sum()\n",
    "        a = np.random.choice(q, p=freqs)\n",
    "        if a != q-1:\n",
    "            new_data[i*(q-1) + a] = 1\n",
    "    return new_data\n",
    "\n",
    "def compute_correlation_matrix(data_points):\n",
    "    \"\"\"Compute correlation matrix, provided that the\n",
    "    data points are given as binary occurrence variables.\"\"\"\n",
    "    M = data_points.shape[0]\n",
    "    x = data_points.astype(np.float32)\n",
    "    xt = x.transpose()\n",
    "    c = np.dot(xt, x) / M\n",
    "    return c\n",
    "\n",
    "def connect_correlation(corr_mat, freqs):\n",
    "    \"\"\"Given the correlation matrix and the frequencies,\n",
    "    return the connected correlation matrix.\"\"\"\n",
    "    c_connected = corr_mat - np.outer(freqs, freqs)\n",
    "    return c_connected\n",
    "\n",
    "def flatten_correlation_matrix(corr, q=21):\n",
    "    \"\"\"Take from the correlation matrix the entries\n",
    "    which do not only depend on 1-pt frequencies.\"\"\"\n",
    "    L = int(corr.shape[0] / (q-1))\n",
    "    flatten = []\n",
    "    for i in range(L):\n",
    "        for a in range(q-1):\n",
    "            flatten = np.r_[flatten, corr[i*(q-1)+a,(i+1)*(q-1):]]\n",
    "    return flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4026270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"train\" model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511c94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d369855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute energies test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71fe4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d72e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1000 sequences from the independent site model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "989138cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute 1-pt and 2-pt correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce9d031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove connected part and \"diagonal\" elements in 2-pt correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d76b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots of model vs data correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4869749b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aadd429b",
   "metadata": {},
   "source": [
    "# part III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239058b",
   "metadata": {},
   "source": [
    "Now we turn to models able to capture 2-point correlations. We consider the model (Boltzmann Machine):\n",
    "$$ p(x) = \\frac{1}{Z} e^{-E(s)}, \\quad E(s) = \\sum_{i, a, j, b} s_{i}(a) \\, J_{i,j}(a,b) \\, s_{j}(b) + \\sum_{i, a} \\, h_i(a) s_i(a), $$\n",
    "where $s_{i}(a)$ is a binary (one-hot) variable.\n",
    "In this case the exact inference of the model (that is, the problem of maximizing the log-likelihood of the data) cannot be solved exactly in a reasonable amount of time. \n",
    "We will consider here the Gaussian approximation ([here](https://iopscience.iop.org/article/10.1088/1361-6633/aa9965/meta) many other approaches are described, and compared), which considers an approximation of the problem that can be solved quickly, provided that the categorical variables have zero average. To do that, we consider the new model:\n",
    "$$ p(x) = \\frac{1}{Z} e^{-E(\\tilde{s})}, \\quad E(\\tilde{s}) = \\sum_{i, a, j, b} \\tilde{s}_{i}(a) \\, J_{i,j}(a,b) \\, \\tilde{s}_{j}(b) + \\sum_{i, a} \\, h_i(a) \\tilde{s}_i(a), $$\n",
    "where $\\tilde{s}_i(a) = s_i - f_i(a)$. Notice that, once we know how to infer the parameters for this new model, the old one can be immediately recovered by inserting the expression for the variables $\\tilde{s}_i(a)$.\n",
    "The Gaussian approximation consists in neglecting the binary nature of the categorical variables, and in computing the parameters $J_{i,j}(a,b)$ and $h_i(a)$ as if they were continuous variables. The relevant advantage in this case is that the normalization $Z$ can be computed exactly and derived with respect to the parameters during the maximization of the log-likelihood, and the solution is:\n",
    "$$ h_i(a) = 0, \\quad J_{i,j}(a,b) = - \\tilde{C}^{-1}_{ia, jb}, $$\n",
    "where $\\tilde{C}^{-1}_{ia, jb}$ is the inverse of the correlation matrix of the (shifted) categorical variables, that is\n",
    "$$ \\tilde{C}_{ia, jb} = \\frac{1}{M} \\sum_s \\tilde{s}_i(a) \\tilde{s}_j(b), $$\n",
    "where $M$ is the number of sequences in the training dataset and the sum spans the sequences of the training dataset. Notice that this correlation matrix can be written as\n",
    "$$ \\tilde{C}_{ia, jb} = \\frac{1}{M} \\sum_s s_i(a) s_j(b) - f_i(a) f_j(b) = C_{ia, jb} - f_i(a) f_j(b), $$\n",
    "where $C_{ia, jb}$ is the correlation matrix of the original variables.\n",
    "\n",
    "Unfortunately, the correlation matrix as it is cannot be inverted whenever the number of data is low (lower than the lenght of each the sequence in categorical variables after gauge fixing) or if the data are correlated. In this case we need again to rely on a regularization method to overcome the problem. We can again use pseudo-counts, transforming the frequences as we did for the indipendent-site model and the 2-point correlations as\n",
    "$$ f_{ij}(a,b) \\to \\tilde{f}_{ij}(a,b) = (1-\\alpha) f_{ij}(a,b) + \\frac{\\alpha}{q^2}, $$\n",
    "where $f_{ij}(a,b)$ is the frequency of having the amino-acid $a$ in position $i$ and $b$ in position $j$.\n",
    "Unfortunately, this time a little extra trick is needed to fix things up after this transformation. Indeed, this transformation has the unwanted properties that now $\\tilde{f}_{ii}(a,b) \\neq 0$ for $a \\neq b$, and that $\\tilde{f}_{ii}(a,a) \\neq f_i(a)$. To get meaningful, regularized frequencies and 2-point correlations, both of these properties must be re-imposed *a posteriori* after applying the transformation.\n",
    "\n",
    "#### Task III.1: fit a Gaussian Boltzmann Machine\n",
    "* Compute the frequencies and the correlation matrix of these variables.\n",
    "* Apply the transformations discussed to regularize these quantities, then take the connected correlation matrix.\n",
    "* Invert the resulting matrix (you can use the function `numpy.linalg.inv` from numpy) to get the $J$ matrix of the model.\n",
    "* Compute the energy of each sequence in the training set, and check whether the new model can discriminate between functional and non-functional sequences as we did for the independent-site model.\n",
    "\n",
    "#### Task III.2: sample from Gaussian Boltzmann Machine to check 1-pt and 2-pt correlations\n",
    "As for the independent-site model, the Gaussian Boltzmann Machine model define a probability distribution over the sequences, and hence can be used to generate new sequences. However, this is much more difficult due to the fact that $Z$ cannot be computed easily (when the variables are back to their binary nature), and that it is not known how to sample directly from the quadratic model (again, when the variables are binary). To overcome this problem, we can use a Metropolis routine to sample from the model. Compare the 1-pt and 2-pt correlations for the training data and those sampled with the Gaussian BM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "757b138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm_energy(jcoupling_gauss, ave_freqs, seq):\n",
    "    \"\"\"Compute energy of seq with the BM model.\"\"\"\n",
    "    # collect local fileds\n",
    "    h = np.dot(jcoupling_gauss, ave_freqs)\n",
    "    ener1 = - np.dot(h, seq)\n",
    "    ener2 = np.dot(seq, np.dot(jcoupling_gauss, seq)) / 2\n",
    "    return -(ener1 + ener2)\n",
    "\n",
    "def add_pseudocounts(corr_mat, freqs, alpha, q_true=21):\n",
    "    \"\"\"Add pseudocounts according to the \n",
    "    parameter alpha to the frequencies and the correlation matrix,\n",
    "    and return them.\"\"\"\n",
    "    q_gauge = q_true - 1\n",
    "    N = len(freqs) // q_gauge\n",
    "    f_pseudo = (1-alpha)*freqs + alpha/q_true\n",
    "    c_pseudo = (1-alpha)*corr_mat + alpha/(q_true**2)\n",
    "    #Fix the conservation constraints in the diagonal blocks\n",
    "    for i in range(N):\n",
    "        c_pseudo[i*q_gauge:(i+1)*q_gauge,i*q_gauge:(i+1)*q_gauge] = 0\n",
    "        for a in range(q_gauge):\n",
    "            c_pseudo[i*q_gauge+a,i*q_gauge+a] = f_pseudo[i*q_gauge+a]\n",
    "    return f_pseudo, c_pseudo\n",
    "\n",
    "def metropolis(jcoupl, p, Nsteps=10000, beta=1, q_true = 21, start_conf=np.nan):\n",
    "    \"\"\"Handle a single chain, starting in start_conf configuration, doing Nsteps steps \n",
    "    (each step is a single, random site flip).\"\"\"\n",
    "    L = len(p)\n",
    "    # prepare initial config\n",
    "    if np.isnan(np.sum(start_conf)):\n",
    "        s = np.random.randint(1, size=L)\n",
    "        start_conf = s.copy()\n",
    "    else:\n",
    "        if type(start_conf[0]) != np.int32:\n",
    "            print('Please use starting configuration in numeric form or use None for random start.')\n",
    "            return 1\n",
    "        else:\n",
    "            s = start_conf\n",
    "    # metropolis\n",
    "    for k in range(0, Nsteps):\n",
    "        # propose move\n",
    "        pos_t = np.random.randint(len(p)//(q_true-1))\n",
    "        t_n = np.random.randint(q_true)\n",
    "        t_1hot = np.repeat(0, q_true-1)\n",
    "        if t_n == q_true-1:\n",
    "            pass\n",
    "        else:\n",
    "            t_1hot[t_n] = 1\n",
    "        s_t = s.copy()\n",
    "        s_t[pos_t*(q_true-1):(pos_t+1)*(q_true-1)] = t_1hot\n",
    "        \n",
    "        \n",
    "        ## faster way to compute energy difference between configurations\n",
    "        h = np.dot(jcoupl[pos_t*(q_true-1):(pos_t+1)*(q_true-1)] , p)\n",
    "        delta_h = - np.dot( (s-s_t)[pos_t*(q_true-1):(pos_t+1)*(q_true-1)], h )\n",
    "        jcoupl_diag = jcoupl[pos_t*(q_true-1):(pos_t+1)*(q_true-1),pos_t*(q_true-1):(pos_t+1)*(q_true-1)]\n",
    "        jcoupl_marginal = np.dot( jcoupl[pos_t*(q_true-1):(pos_t+1)*(q_true-1)] , s)\n",
    "        delta_seq = (s-s_t)[pos_t*(q_true-1):(pos_t+1)*(q_true-1)]\n",
    "        delta_coupl1 = np.dot( delta_seq , jcoupl_marginal  )\n",
    "        delta_coupl2 = - 0.5 * np.dot( delta_seq, np.dot(jcoupl_diag, delta_seq)  )\n",
    "        energy_diff = delta_h + delta_coupl1 + delta_coupl2\n",
    "        \n",
    "        ## slower (but simpler to read) way to compute energy difference between configurations\n",
    "        #energy_diff = bm_energy(jcoupl, p, s_t) - bm_energy(jcoupl, p, s)\n",
    "        \n",
    "        # accept or not\n",
    "        prob = min(1, np.exp(- beta * (energy_diff)))\n",
    "        x = np.random.rand()\n",
    "        if x < prob:\n",
    "            s = s_t.copy()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ec65a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute (non connected) correlation matrix\n",
    "\n",
    "# add pseudocount and take connected matrix\n",
    "\n",
    "# compute coupling matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1eb92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute energies test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2abdf53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram & compute AUROC test set (global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "578640ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from gaussian BM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6530502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed12d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sampling by comparing with the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d409c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute 1-pt and 2-pt correlations of the sampled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d45a82c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plots of model vs data correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385596f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88579c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b36c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
